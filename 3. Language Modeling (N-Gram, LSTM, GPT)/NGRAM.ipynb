{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import product\n",
    "import math\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import itertools\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"green\">Tokenization</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TOKENIZERS = False\n",
    "TRAIN_MODEL = False\n",
    "\n",
    "WORD_TOKENIZER_FILE_NAME = './wtoken.json'\n",
    "BPE_TOKENIZER_FILE_NAME = './bpetoken.json'\n",
    "\n",
    "BPE_VOCAB_SIZE = 10000\n",
    "WORD_LEVEL_VOCAB_SIZE = 5000\n",
    "\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "\n",
    "ALL_TOKENS = [UNK_TOKEN, SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]\n",
    "\n",
    "ALL_TRAINING_DATA = [\n",
    "    'data/cultural.txt',\n",
    "    'data/economics.txt',\n",
    "    'data/politics.txt',\n",
    "    'data/sports.txt'\n",
    "]\n",
    "\n",
    "LM_TRAINING_DATA = ['./train.txt'] # ALL_TRAINING_DATA[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"blue\">Word Tokenizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZERS:\n",
    "    word_tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
    "    word_tokenizer.pre_tokenizer = Whitespace()\n",
    "    word_trainer = WordLevelTrainer(vocab_size=WORD_LEVEL_VOCAB_SIZE, special_tokens=ALL_TOKENS)\n",
    "    word_tokenizer.train(ALL_TRAINING_DATA, word_trainer)\n",
    "    word_tokenizer.enable_padding(pad_token=PAD_TOKEN)\n",
    "    word_tokenizer.save(WORD_TOKENIZER_FILE_NAME)\n",
    "else:\n",
    "    word_tokenizer = Tokenizer.from_file(WORD_TOKENIZER_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"blue\">BPE Tokenizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZERS:\n",
    "    bpe_tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN))\n",
    "    bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "    bpe_trainer = BpeTrainer(vocab_size=BPE_VOCAB_SIZE, special_tokens=ALL_TOKENS)\n",
    "    bpe_tokenizer.train(ALL_TRAINING_DATA, bpe_trainer)\n",
    "    bpe_tokenizer.enable_padding(pad_token=PAD_TOKEN)\n",
    "    bpe_tokenizer.save(BPE_TOKENIZER_FILE_NAME)\n",
    "else:\n",
    "    bpe_tokenizer = Tokenizer.from_file(BPE_TOKENIZER_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"blue\">Post Processing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_post_processor_to(tokenizer: Tokenizer):\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{SOS_TOKEN} $0 {EOS_TOKEN}\",\n",
    "        special_tokens=[\n",
    "            (X, tokenizer.token_to_id(X)) for X in [SOS_TOKEN, EOS_TOKEN]\n",
    "        ]\n",
    "    )\n",
    "add_post_processor_to(word_tokenizer)\n",
    "add_post_processor_to(bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer: ['[SOS]', 'سلاااااام', 'حالت', 'خوب', 'است', '؟', '[EOS]']\n",
      "BPE Tokenizer: ['[SOS]', 'س', 'لا', 'ا', 'ا', 'ا', 'ا', 'ام', 'حالت', 'خوب', 'است', '؟', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "sample = 'سلاااااام حالت خوب است؟'\n",
    "print(f'Word Tokenizer: {word_tokenizer.encode(sample).tokens}')\n",
    "print(f'BPE Tokenizer: {bpe_tokenizer.encode(sample).tokens}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class LanguageModel(object):\n",
    "    def __init__(self, train_data, n, laplace, tokenizer):\n",
    "        self.set_tokenizer(tokenizer)\n",
    "        self.n = n\n",
    "        self.vocab = dict()\n",
    "        self.laplace = laplace\n",
    "        self.tokens = self.preprocess(train_data, n)\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "    \n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.sos = str(tokenizer.token_to_id(SOS_TOKEN))\n",
    "        self.eos = str(tokenizer.token_to_id(EOS_TOKEN))\n",
    "        self.unk = str(tokenizer.token_to_id(UNK_TOKEN))\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def _smooth(self):\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def _create_model(self):\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _convert_oov(self, ngram):\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else self.unk for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        test_tokens = self.preprocess(test_data, self.n)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "\n",
    "        known_ngrams  = [self._convert_oov(ngram) for ngram in test_ngrams]\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "        \n",
    "#         for x,y in zip(known_ngrams, probabilities):\n",
    "#             print(x,y)\n",
    "        \n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev, without=[]):\n",
    "        \n",
    "        blacklist  = [self.unk] + without\n",
    "\n",
    "        if len(prev) < self.n:\n",
    "            prev = [self.sos]*(self.n-1)\n",
    "\n",
    "        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==tuple(prev)))\n",
    "\n",
    "        probs = [y for x,y in candidates]\n",
    "        probs = probs/np.sum(probs)\n",
    "        words = [x for x,y in candidates]\n",
    "\n",
    "        idx = np.random.choice(len(words), 1, replace=False, p=probs)[0]\n",
    "        \n",
    "        while words[idx] in blacklist:\n",
    "            idx = np.random.choice(len(words), 1, replace=False, p=probs)[0]\n",
    "        \n",
    "        return (words[idx], probs[idx])\n",
    "         \n",
    "    def generate_sentence(self, min_len=12, max_len=24):\n",
    "        sent, prob = ([self.sos] * (max(1, self.n-1)), 1)\n",
    "        while sent[-1] != self.eos:\n",
    "            prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "            blacklist = sent + ([self.eos,self.sos] if len(sent) < min_len else [])\n",
    "            next_token, next_prob = self._best_candidate(prev, without=blacklist)\n",
    "            sent.append(next_token)\n",
    "            prob *= next_prob\n",
    "\n",
    "            if len(sent) >= max_len:\n",
    "                sent.append(self.eos)\n",
    "\n",
    "        return (' '.join(sent[(self.n-1):-1]), -1/math.log(prob))\n",
    "    \n",
    "    \n",
    "    def add_sentence_tokens(self, sentences, n):\n",
    "        return_value = []\n",
    "        sos = ' '.join(self.sos * (n-1)) if n > 1 else self.sos\n",
    "        for sentence in sentences:\n",
    "            ids = self.tokenizer.encode(sentence).ids\n",
    "            sos_id = ids[0]\n",
    "            ids = ids[1:]\n",
    "            s = ' '.join([str(x) for x in ids])\n",
    "            return_value.append('{} {}'.format(sos, s))\n",
    "        return return_value\n",
    "\n",
    "\n",
    "    def preprocess(self, sentences, n):\n",
    "        sentences = self.add_sentence_tokens(sentences, n)\n",
    "        tokens = ' '.join(sentences).split()\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(train_data):\n",
    "    trains = []\n",
    "    for i in range(len(train_data)):\n",
    "        with open(train_data[i], 'r', encoding=\"utf-8\") as f:\n",
    "            trains.append([l.strip() for l in f.readlines()])\n",
    "            \n",
    "    train = [item for sublist in trains for item in sublist]\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def train_ngram_and_save(n, laplace, name, tokenizer):\n",
    "    train = load_train_data(LM_TRAINING_DATA)\n",
    "    lm = LanguageModel(train, n, laplace, tokenizer)\n",
    "\n",
    "    with open(f'ngram_{name}_{n}_{laplace}.pkl', 'wb') as outp:\n",
    "        pickle.dump(lm, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram_and_save(3, 0, 'word', word_tokenizer)\n",
    "train_ngram_and_save(3, 0, 'bpe', bpe_tokenizer)\n",
    "train_ngram_and_save(3, 1, 'word', word_tokenizer)\n",
    "train_ngram_and_save(3, 1, 'bpe', bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
