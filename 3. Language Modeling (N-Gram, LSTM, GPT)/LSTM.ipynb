{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326f1349-a608-490e-a8a4-b0a6d169928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TOKENIZERS = False\n",
    "\n",
    "WORD_TOKENIZER_FILE_NAME = './wtoken.json'\n",
    "BPE_TOKENIZER_FILE_NAME = './bpetoken.json'\n",
    "\n",
    "BPE_VOCAB_SIZE = 10000\n",
    "WORD_LEVEL_VOCAB_SIZE = 5000\n",
    "\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "ALL_TOKENS = [UNK_TOKEN, SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]\n",
    "\n",
    "ALL_TRAINING_DATA = [\n",
    "    './cultural.txt',\n",
    "    './economics.txt',\n",
    "    './politics.txt',\n",
    "    './sports.txt'\n",
    "]\n",
    "\n",
    "LM_TRAINING_DATA = ['./t.txt'] #ALL_TRAINING_DATA[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc81cf-ea30-48dd-be44-d02a177ae4d5",
   "metadata": {},
   "source": [
    "# <div class=\"green\">Tokenization</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d300f43d-9680-4641-baa4-5c3fa52532f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e48e2c-c23f-4802-88ae-6bd7bdd94419",
   "metadata": {},
   "source": [
    "## <span class=\"blue\">Word Tokenizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3890c8-bc45-4e16-aa52-182036f93ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZERS:\n",
    "    word_tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
    "    word_tokenizer.pre_tokenizer = Whitespace()\n",
    "    word_trainer = WordLevelTrainer(vocab_size=WORD_LEVEL_VOCAB_SIZE, special_tokens=ALL_TOKENS)\n",
    "    word_tokenizer.train(ALL_TRAINING_DATA, word_trainer)\n",
    "    word_tokenizer.enable_padding(pad_token=PAD_TOKEN)\n",
    "    word_tokenizer.save(WORD_TOKENIZER_FILE_NAME)\n",
    "else:\n",
    "    word_tokenizer = Tokenizer.from_file(WORD_TOKENIZER_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1113abc9-2c8f-4a4e-8e06-2206d42135fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tokenizer.id_to_token(self, id)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer.id_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382e0da-2509-438c-9e1b-647c67538997",
   "metadata": {},
   "source": [
    "## <span class=\"blue\">BPE Tokenizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237ecc7a-da50-4a13-a6be-df271c62e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZERS:\n",
    "    bpe_tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN))\n",
    "    bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "    bpe_trainer = BpeTrainer(vocab_size=BPE_VOCAB_SIZE, special_tokens=ALL_TOKENS)\n",
    "    bpe_tokenizer.train(ALL_TRAINING_DATA, bpe_trainer)\n",
    "    bpe_tokenizer.enable_padding(pad_token=PAD_TOKEN)\n",
    "    bpe_tokenizer.save(BPE_TOKENIZER_FILE_NAME)\n",
    "else:\n",
    "    bpe_tokenizer = Tokenizer.from_file(BPE_TOKENIZER_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7f475-4173-43d2-9fd5-1c00d0a32e9c",
   "metadata": {},
   "source": [
    "## <span class=\"blue\">Post Processing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f05b39-4337-43b6-a82c-2d1e031a625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_post_processor_to(tokenizer: Tokenizer):\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{SOS_TOKEN} $0 {EOS_TOKEN}\",\n",
    "        special_tokens=[\n",
    "            (X, tokenizer.token_to_id(X)) for X in [SOS_TOKEN, EOS_TOKEN]\n",
    "        ]\n",
    "    )\n",
    "add_post_processor_to(word_tokenizer)\n",
    "add_post_processor_to(bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8912850e-d2d4-4056-b6f1-95a799d4fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, lines, bsz:int, tokenizer, tokenizer_args):                \n",
    "        lines_tokenized = [tokenizer(line, **tokenizer_args)['input_ids'][0] for line in lines]\n",
    "        self.__lines = []\n",
    "        for i in tqdm.tqdm(range(0, len(lines_tokenized), bsz)):\n",
    "            tensor = torch.stack(lines_tokenized[i: i+bsz]).T.to('cuda')\n",
    "            self.__lines.append(tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor = self.__lines[idx]\n",
    "        return tensor[:-1], tensor[1:].reshape(-1)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus_files):\n",
    "        dataset_lines = []\n",
    "\n",
    "        for file_name in LM_TRAINING_DATA:\n",
    "            with open(file_name, 'r') as f:\n",
    "                dataset_lines += f.readlines()\n",
    "        dataset_lines = [line.strip() for line in dataset_lines]\n",
    "                \n",
    "        self.__lines = dataset_lines\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.__lines[idx]\n",
    "    \n",
    "    def get_tokenized(self, bsz, tokenizer, **tokenizer_args):\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=copy.deepcopy(tokenizer))\n",
    "        fast_tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})\n",
    "        return TokenizedTextDataset(self.__lines, bsz, fast_tokenizer, tokenizer_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38c3c45-e963-4640-bc3f-b5f15e30efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(LM_TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085d8a4c-5074-47d1-a99d-f2fb941042e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, bsz):\n",
    "        super().__init__()\n",
    "        dropout=0.5\n",
    "        nlayers = 1\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.bsz = bsz\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, self.bsz, self.nhid), weight.new_zeros(self.nlayers, self.bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ac2f93-b218-443a-b14b-e2571bd06054",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "MAX_LENGTH = 128\n",
    "BPTT = 32\n",
    "CLIP = 0.25\n",
    "HIDDEN_SIZE = 100\n",
    "EMBEDING_SIZE = 100\n",
    "BATCH_SIZE = 20\n",
    "INITIAL_LR = 20\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07fc70a-7fe6-49ae-9cde-6af11851efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach().to('cuda')\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def train_epoch(epoch, tokenized_dataset:TokenizedTextDataset, model, criterion, batch_size, log_interval, lr):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden()\n",
    "    for batch_idx in range(0, len(tokenized_dataset)):\n",
    "        data, targets = tokenized_dataset[batch_idx]\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch_idx, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69af50aa-4ad4-4bb4-9a08-3b3f04f94180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "def train_all(dataset: TextDataset, tokenizer, save_interval, save_patch):\n",
    "    dataset_tokenized = dataset.get_tokenized(BATCH_SIZE, tokenizer, truncation='longest_first', return_tensors=\"pt\", max_length=MAX_LENGTH, padding='max_length')\n",
    "    best_val_loss = None\n",
    "    \n",
    "    n_tokens = tokenizer.get_vocab_size()\n",
    "    model = LSTMModel(n_tokens, EMBEDING_SIZE, HIDDEN_SIZE, BATCH_SIZE)\n",
    "    model.to('cuda')\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch(epoch, dataset_tokenized, model, criterion, BATCH_SIZE, 1000, INITIAL_LR)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s |'.format(epoch, (time.time() - epoch_start_time)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if epoch % save_interval == 0:\n",
    "            with open(save_patch, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "    with open(save_patch, 'wb') as f:\n",
    "        torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d34488bf-2132-48c1-8a49-4eef27f7c18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 912.67it/s]\n",
      "/home/mohalisad/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000 batches | lr 20.00 | ms/batch 10.01 | loss  2.86 | ppl    17.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 19.98s |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000 batches | lr 20.00 | ms/batch  9.98 | loss  2.56 | ppl    12.93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9ba792b6cdda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./lstm_word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c7333ef14900>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(dataset, tokenizer, save_interval, save_patch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINITIAL_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'| end of epoch {:3d} | time: {:5.2f}s |'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9e2aaaa328a9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, tokenized_dataset, model, criterion, batch_size, log_interval, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_all(dataset, word_tokenizer, 5, './lstm_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2851e72-20af-44e0-aa47-9814ecc64dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 22863.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000 batches | lr 20.00 | ms/batch 15.33 | loss  3.77 | ppl    43.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 30.61s |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000 batches | lr 20.00 | ms/batch 15.30 | loss  3.41 | ppl    30.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 30.57s |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-db6530570102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpe_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./lstm_bpe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c7333ef14900>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(dataset, tokenizer, save_interval, save_patch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINITIAL_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'| end of epoch {:3d} | time: {:5.2f}s |'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9e2aaaa328a9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, tokenized_dataset, model, criterion, batch_size, log_interval, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_all(dataset, bpe_tokenizer, 5, './lstm_bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16c15c-0847-42e6-a23f-5b5077909d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
