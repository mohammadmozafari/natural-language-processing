{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c679b61-f179-4b3e-adf9-03a53dd87f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
       "<link rel=\"stylesheet\" href=\"style.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "<link rel=\"stylesheet\" href=\"style.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bce71f-854f-4d63-9f6d-32bba0482b38",
   "metadata": {},
   "source": [
    "# <div class=\"farsi center\">بسم الله الرحمن الرحیم</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990b98d-1492-43c9-be93-160f2cbaf97c",
   "metadata": {},
   "source": [
    "## <div class=\"farsi right\">محمد علی صدرایی- 400210993</div>\n",
    "## <div class=\"farsi right\">محمد مظفری- 400201167</div>\n",
    "## <div class=\"farsi right\">علیرضا زارع نژاد- 400201101</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50976855-335a-4496-b125-5d5c2fbd8b66",
   "metadata": {},
   "source": [
    "# <div class=\"farsi right green\"> تعریف توکنایزر‌ها </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4475be9-c215-4fef-996a-fb8edee87344",
   "metadata": {},
   "source": [
    "<div class=\"farsi right\"> برای توکنایز‌ها از توکنایزر‌های کتابخانه Huggingface استفاده شده است.دو توکنایزر در این پروژه استفاده شده است یک توکنایزر در سطح کلمه و یک توکنایزر با استفاده از bpe. </div>\n",
    "<div class=\"farsi right\"> برای آموزش دادن توکنایزر‌ها از کل ۴ فایل داده شده برای پروژه استفاده شده است. همچنین حداکثر تعداد توکن‌ها برای سطح کلمه ۵۰۰۰ و برای سطح bpe برابر با ۱۰۰۰۰ گذاشته شده است. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31488739-a742-4091-871e-35f89eb47364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6be3ce0-81fd-4c0a-9311-e557f14ce96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TOKENIZERS = False\n",
    "\n",
    "WORD_TOKENIZER_FILE_NAME = './wtoken.json'\n",
    "BPE_TOKENIZER_FILE_NAME = './bpetoken.json'\n",
    "\n",
    "BPE_VOCAB_SIZE = 10000\n",
    "WORD_LEVEL_VOCAB_SIZE = 5000\n",
    "\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "ALL_TOKENS = [UNK_TOKEN, SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]\n",
    "\n",
    "ALL_TRAINING_DATA = [\n",
    "    './cultural.txt',\n",
    "    './economics.txt',\n",
    "    './politics.txt',\n",
    "    './sports.txt'\n",
    "]\n",
    "\n",
    "LM_TRAINING_DATA = ['./train.txt']\n",
    "LM_TEST_DATA = ['./test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a9b99d-7a95-4a83-ba72-af433ab88bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "test_results = defaultdict(dict)\n",
    "train_results = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227aab4e-2494-4e34-a598-252fc79eae7f",
   "metadata": {},
   "source": [
    "# <div class=\"green\">Tokenization</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b851f2-4c9a-47c2-a3ee-3637dc998d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e1dcc4-5372-4eac-97d0-cad3808f0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer.from_file(WORD_TOKENIZER_FILE_NAME)\n",
    "bpe_tokenizer = Tokenizer.from_file(BPE_TOKENIZER_FILE_NAME)\n",
    "def add_post_processor_to(tokenizer: Tokenizer):\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{SOS_TOKEN} $0 {EOS_TOKEN}\",\n",
    "        special_tokens=[\n",
    "            (X, tokenizer.token_to_id(X)) for X in [SOS_TOKEN, EOS_TOKEN]\n",
    "        ]\n",
    "    )\n",
    "    tokenizer.enable_truncation(128)\n",
    "\n",
    "add_post_processor_to(word_tokenizer)\n",
    "add_post_processor_to(bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c86f86-90cc-40b0-91a0-80067acf9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {'word': word_tokenizer, 'bpe': bpe_tokenizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d4b99-2c77-440d-b865-01346578cee9",
   "metadata": {},
   "source": [
    "# <div class=\"farsi right green\"> خواندن داده‌های تست </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5bb2b0-c7b7-46c2-b12b-dd51d77829aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus_files):\n",
    "        dataset_lines = []\n",
    "\n",
    "        for file_name in corpus_files:\n",
    "            with open(file_name, 'r') as f:\n",
    "                dataset_lines += f.readlines()\n",
    "        dataset_lines = [line.strip() for line in dataset_lines]\n",
    "                \n",
    "        self.__lines = dataset_lines\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.__lines[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b3c725-6eb3-4f67-a6e4-3ab3850ba213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(LM_TRAINING_DATA)\n",
    "test_dataset = TextDataset(LM_TEST_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c612a6-4c2d-4d0d-9672-e237604cc841",
   "metadata": {},
   "source": [
    "# <div class=\"farsi right green\"> لود کردن مدل‌های n-gram و محاسبه پرپلکسیتی آن </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a5853-8188-40eb-802f-9f5e6c3ef1df",
   "metadata": {},
   "source": [
    "<div class=\"farsi right\">دو مدل n-gram در این پروژه ترین شد. یک مدل با n=3 و laplace=0 و یک مدل با n=3 و laplace=1</div>\n",
    "<div class=\"farsi right\">در این بخش فایل pkl از پیش ترین شده آن‌ها را لود میکنیم و پرپلکسیتی داده تست را به وسیله آن‌ها محاسبه می‌کنیم.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7ff4bd0-431d-4f3a-9bf7-c05fd2897b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import product\n",
    "import math\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import itertools\n",
    "import codecs\n",
    "\n",
    "class LanguageModel(object):\n",
    "    def __init__(self, train_data, n, laplace, tokenizer):\n",
    "        self.set_tokenizer(tokenizer)\n",
    "        self.n = n\n",
    "        self.vocab = dict()\n",
    "        self.laplace = laplace\n",
    "        self.tokens = self.preprocess(train_data, n)\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "    \n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.sos = str(tokenizer.token_to_id(SOS_TOKEN))\n",
    "        self.eos = str(tokenizer.token_to_id(EOS_TOKEN))\n",
    "        self.unk = str(tokenizer.token_to_id(UNK_TOKEN))\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def _smooth(self):\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def _create_model(self):\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _convert_oov(self, ngram):\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else self.unk for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        test_tokens_list = self.add_sentence_tokens(test_data, self.n)\n",
    "        total_len = 0\n",
    "        total_prob = 0\n",
    "        \n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        for test_tokens in tqdm.tqdm(test_tokens_list):\n",
    "            test_tokens = test_tokens.split()\n",
    "            test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "\n",
    "            \n",
    "            known_ngrams  = [self._convert_oov(ngram) for ngram in test_ngrams]\n",
    "            probabilities = [self.model[ngram]  for ngram in known_ngrams if ngram is not None]\n",
    "                \n",
    "            total_len += len(probabilities)\n",
    "            total_prob += sum(map(math.log, probabilities))\n",
    "            #for x,y in zip(known_ngrams, probabilities):\n",
    "             #   print(x,y)\n",
    "        \n",
    "        return math.exp((-1/total_len) * total_prob)\n",
    "\n",
    "    def _best_candidate(self, prev, without=[]):\n",
    "        \n",
    "        blacklist  = [self.unk] + without\n",
    "\n",
    "        if len(prev) < self.n:\n",
    "            prev = [self.sos]*(self.n-1)\n",
    "\n",
    "        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==tuple(prev)))\n",
    "\n",
    "        probs = [y for x,y in candidates]\n",
    "        probs = probs/np.sum(probs)\n",
    "        words = [x for x,y in candidates]\n",
    "\n",
    "        idx = np.random.choice(len(words), 1, replace=False, p=probs)[0]\n",
    "        \n",
    "        while words[idx] in blacklist:\n",
    "            idx = np.random.choice(len(words), 1, replace=False, p=probs)[0]\n",
    "        \n",
    "        return (words[idx], probs[idx])\n",
    "         \n",
    "    def generate_sentence(self, min_len=12, max_len=24):\n",
    "        sent, prob = ([self.sos] * (max(1, self.n-1)), 1)\n",
    "        while sent[-1] != self.eos:\n",
    "            prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "            blacklist = sent + ([self.eos,self.sos] if len(sent) < min_len else [])\n",
    "            next_token, next_prob = self._best_candidate(prev, without=blacklist)\n",
    "            sent.append(next_token)\n",
    "            prob *= next_prob\n",
    "\n",
    "            if len(sent) >= max_len:\n",
    "                sent.append(self.eos)\n",
    "\n",
    "        return (' '.join(sent[(self.n-1):-1]), -1/math.log(prob))\n",
    "    \n",
    "    \n",
    "    def add_sentence_tokens(self, sentences, n):\n",
    "        return_value = []\n",
    "        sos = ' '.join(self.sos * (n-1)) if n > 1 else self.sos\n",
    "        for sentence in sentences:\n",
    "            ids = self.tokenizer.encode(sentence).ids\n",
    "            sos_id = ids[0]\n",
    "            ids = ids[1:]\n",
    "            s = ' '.join([str(x) for x in ids])\n",
    "            return_value.append('{} {}'.format(sos, s))\n",
    "        return return_value\n",
    "\n",
    "\n",
    "    def preprocess(self, sentences, n):\n",
    "        sentences = self.add_sentence_tokens(sentences, n)\n",
    "        tokens = ' '.join(sentences).split()\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "141346f3-0268-42fd-a2c6-4d178095a40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_word_3_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:14<00:00, 2785.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_bpe_3_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:15<00:00, 2604.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_word_3_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:14<00:00, 2785.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_bpe_3_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:15<00:00, 2592.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for ngram_model_file in glob.glob('ngram*.pkl'):\n",
    "    print(ngram_model_file)\n",
    "    name_parts = ngram_model_file[:-4].split('_')\n",
    "    level = name_parts[1]\n",
    "    n = int(name_parts[2])\n",
    "    laplace = int(name_parts[3])\n",
    "    with open(ngram_model_file, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        model.set_tokenizer(tokenizers[level])\n",
    "        ppl = model.perplexity(train_dataset)\n",
    "        train_results[f'ngram_{n}_{laplace}'][level] = ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07dae23f-04a0-42ff-afe4-0b7f8f1f0ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_word_3_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:25<00:00, 3163.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_bpe_3_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:27<00:00, 2932.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_word_3_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:25<00:00, 3154.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_bpe_3_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:27<00:00, 2931.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for ngram_model_file in glob.glob('ngram*.pkl'):\n",
    "    name_parts = ngram_model_file[:-4].split('_')\n",
    "    level = name_parts[1]\n",
    "    n = int(name_parts[2])\n",
    "    laplace = int(name_parts[3])\n",
    "    with open(ngram_model_file, 'rb') as f:\n",
    "        print(ngram_model_file)\n",
    "        model = pickle.load(f)\n",
    "        model.set_tokenizer(tokenizers[level])\n",
    "        ppl = model.perplexity(test_dataset)\n",
    "        test_results[f'ngram_{n}_{laplace}'][level] = ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69a904-1da5-4090-a166-5069854eb693",
   "metadata": {},
   "source": [
    "# <div class=\"farsi right green\"> لود کردن مدل LSTM و محاسبه پرپلکسیتی آن </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616f90f-1fd4-4b48-86d7-cdf185efbf97",
   "metadata": {},
   "source": [
    "<div class=\"farsi right\">مدل lstm استفاده شده دارای یک لایه است و طول امبدینگ ۱۰۰ برای آن در نظر گرفته شده است. اندازه لایه مخفی نیز برابر ۱۰۰ در نظر گرفته شده است.</div>\n",
    "<div class=\"farsi right\">برای ترین راحت‌تر مدل از کلیپینگ به همراه دراپ او با احتمال ۵ درصد استفاده شده است.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b829aa44-f33b-4759-b515-b5f2a7d46e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 300\n",
    "MAX_LENGTH = 128\n",
    "BPTT = 32\n",
    "CLIP = 0.25\n",
    "HIDDEN_SIZE = 100\n",
    "EMBEDING_SIZE = 100\n",
    "BATCH_SIZE = 20\n",
    "INITIAL_LR = 20\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, bsz):\n",
    "        super().__init__()\n",
    "        dropout=0.5\n",
    "        nlayers = 1\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.bsz = bsz\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, self.bsz, self.nhid), weight.new_zeros(self.nlayers, self.bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfecfd96-d7e9-4d7e-b4c0-779cbcdae79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lstm_ppl(dataset, model, tokenizer):\n",
    "    nlls = []\n",
    "\n",
    "    sum_len = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    for line in tqdm.tqdm(dataset):\n",
    "        ids = tokenizer.encode(line).ids\n",
    "        input_ids = torch.tensor(ids[:-1]).view(-1, 1).to('cuda')\n",
    "        target_ids = torch.tensor(ids[1:]).to('cuda')\n",
    "        hidden = model.init_hidden()\n",
    "        trg_len = len(ids)\n",
    "        sum_len += trg_len\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(input_ids, hidden)\n",
    "\n",
    "            loss = criterion(outputs, target_ids)\n",
    "            neg_log_likelihood = loss.item() * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "    return torch.exp(torch.tensor(nlls).sum() / sum_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d67b10-994e-41ad-bf10-fa3be251e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:30<00:00, 1330.03it/s]\n",
      "100%|██████████| 40000/40000 [00:35<00:00, 1139.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for mode in ['word', 'bpe']:\n",
    "    path = f'lstm_{mode}'\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    model.bsz = 1\n",
    "    ppl = calc_lstm_ppl(train_dataset, model, tokenizers[mode])\n",
    "    train_results[f'lstm'][mode] = ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "088235f3-84a9-408a-af32-de2e69973d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:52<00:00, 1525.53it/s]\n",
      "100%|██████████| 80000/80000 [01:00<00:00, 1318.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for mode in ['word', 'bpe']:\n",
    "    path = f'lstm_{mode}'\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    model.bsz = 1\n",
    "    ppl = calc_lstm_ppl(test_dataset, model, tokenizers[mode])\n",
    "    test_results[f'lstm'][mode] = ppl.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa9c6-e8f7-49c5-a955-a6de533b9abb",
   "metadata": {},
   "source": [
    "# <div class=\"farsi right green\"> لود کردن مدل GPT2 و محاسبه پرپلکسیتی آن </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d3877-3d87-4b6c-b4af-bcf70d231825",
   "metadata": {},
   "source": [
    "<div class=\"farsi right\">مدل gpt2 استفاده شده در این پروژه یک دیکودر سه لایه است و از سایز امبدینگ ۱۲۰ در آن استفاده شده است.</div>\n",
    "<div class=\"farsi right\">این مدل دارای ۱۲ head برای اتنشن خود است.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8cee566-0d72-4516-87b4-dd784371723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "def calc_gpt2_ppl(dataset, model, tokenizer):\n",
    "    nlls = []\n",
    "    \n",
    "    sum_len = 0\n",
    "\n",
    "    for line in tqdm.tqdm(dataset):\n",
    "        ids = tokenizer.encode(line).ids\n",
    "        input_ids = torch.tensor(ids).to('cuda')\n",
    "        target_ids = input_ids.clone()\n",
    "        trg_len = len(ids)\n",
    "        sum_len += trg_len\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / sum_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc86f8b-6719-4951-8d88-bd3431b52f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [02:14<00:00, 595.88it/s]\n",
      "100%|██████████| 80000/80000 [02:11<00:00, 610.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for mode in ['bpe', 'word']:\n",
    "    model = GPT2LMHeadModel.from_pretrained(f'gpt2_{mode}').to('cuda')\n",
    "    ppl = calc_gpt2_ppl(test_dataset, model, tokenizers[mode])\n",
    "    test_results[f'gpt2'][mode] = ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65b8d08-9909-4282-acef-26d28e797458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [01:09<00:00, 573.41it/s]\n",
      "100%|██████████| 40000/40000 [01:07<00:00, 592.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for mode in ['bpe', 'word']:\n",
    "    model = GPT2LMHeadModel.from_pretrained(f'gpt2_{mode}').to('cuda')\n",
    "    ppl = calc_gpt2_ppl(train_dataset, model, tokenizers[mode])\n",
    "    train_results[f'gpt2'][mode] = ppl.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556d5c3-380a-4dd1-96f9-bd8ccb2a7aa8",
   "metadata": {},
   "source": [
    "# <div class=\"farsi right green\">مقایسه مدل‌ها و نتایج نهایی</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901f9c5-54b5-4724-817a-eb7fb28766c8",
   "metadata": {},
   "source": [
    "### <div class=\"farsi right blue\">پرپلکسیتی داده ترین</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "316e4023-f704-4936-a90f-74210a86702c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>bpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ngram_3_0</th>\n",
       "      <td>17.233117</td>\n",
       "      <td>10.362756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_3_1</th>\n",
       "      <td>637.935506</td>\n",
       "      <td>2058.613123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>83.147423</td>\n",
       "      <td>169.956314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>50.772160</td>\n",
       "      <td>66.034538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word          bpe\n",
       "ngram_3_0   17.233117    10.362756\n",
       "ngram_3_1  637.935506  2058.613123\n",
       "lstm        83.147423   169.956314\n",
       "gpt2        50.772160    66.034538"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(train_results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce67d4-69a1-4088-98d0-6ca415f836dd",
   "metadata": {},
   "source": [
    "### <div class=\"farsi right blue\">پرپلکسیتی داده تست</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a74f63c7-e96c-49b3-b0e0-7e3795aaa6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>bpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ngram_3_0</th>\n",
       "      <td>18.209904</td>\n",
       "      <td>16.458979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_3_1</th>\n",
       "      <td>413.521919</td>\n",
       "      <td>1037.899298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>238.585480</td>\n",
       "      <td>530.619385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>315.204773</td>\n",
       "      <td>640.165161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word          bpe\n",
       "ngram_3_0   18.209904    16.458979\n",
       "ngram_3_1  413.521919  1037.899298\n",
       "lstm       238.585480   530.619385\n",
       "gpt2       315.204773   640.165161"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_dict(test_results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc507c17-889d-4771-88be-9f497e2cb145",
   "metadata": {},
   "source": [
    "<div class=\"farsi right blue\">نتایج کلی:</div>\n",
    "<div class=\"farsi right\">۱- bpe در اکثر اوقات باعث بیشتر شدن پرپلکسیتی شده است.</div>\n",
    "<div class=\"farsi right\">۲-مدل‌های مبتنی بر شبکه عصبی به علت کوچک بودن و ترینینگ کم، ضعیفتر از مدل 3gram بدون لاپلاس عمل می‌کنند.</div>\n",
    "<div class=\"farsi right\">۳- مدل lstm و gpt2 دچار مشکل شدید overfit شده‌اند.</div>\n",
    "<div class=\"farsi right\">۴- مدل دارای لاپلاس دارای بیشترین پرپلکسیتی در هر دو حالت است.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd8d88-c6de-4200-a2c9-0ec1aff924ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
