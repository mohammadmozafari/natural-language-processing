{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75d7af06",
      "metadata": {
        "id": "75d7af06"
      },
      "source": [
        "<img src='https://github.com/language-ml/1-nlp-exploring-datasets/blob/main/notebooks/images/besm.png?raw=1' width='150px'>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d8d23b",
      "metadata": {
        "id": "01d8d23b"
      },
      "source": [
        "<center> درس پردازش زبان‌های طبیعی </center>\n",
        "<center> آزمایشگاه پردازش هوشمند متن و زبان و علوم انسانی محاسباتی </center>\n",
        "<br>\n",
        "<center> http://language.ml </center>\n",
        "<center> contact: asgari [AT] berkeley [dot] edu </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b2faf6d",
      "metadata": {
        "id": "8b2faf6d"
      },
      "source": [
        "<h1 style='direction:rtl'><b>تمرین دوم: پیش پردازش متن</b></h1>\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffefc04",
      "metadata": {
        "id": "3ffefc04"
      },
      "source": [
        "<h5 style='direction:rtl'>نام و نام خانوادگی: محمد مظفری</h2>\n",
        "<h5 style='direction:rtl'>شماره دانشجویی: 400201167</h2>\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b446f5a",
      "metadata": {
        "id": "0b446f5a"
      },
      "source": [
        "<h1 style='direction:rtl;'><b>یک پاراگراف توضیح در مورد متنی که انتخاب کرده اید</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac62627",
      "metadata": {
        "id": "5ac62627"
      },
      "source": [
        "<p style='direction:rtl'>متن انتخاب شده برای این تمرین در 4 فایل مجزا در پوشه dataset کنار این نوتبوک دیده میشود. این مجموعه متن شامل اشعار چهار شاعر است. این شاعران عبارتند از:</p>\n",
        "<ul style='direction:rtl'>\n",
        "    <li> غزلیات حافظ در فایل hafez.txt </li>\n",
        "    <li> اشعاز صائب در فایل saeb.txt</li>\n",
        "    <li> اشعار پروین اعتصامی در فایل parvin.txt</li>\n",
        "    <li> اشعار شهریار در فایل shahriar.txt</li>\n",
        "</ul>\n",
        "<p style='direction:rtl'>مجموعه آموزشی مربوط به 4 شاعر مختلف است. حافظ و صائب شعرای قدیمی تری هستند و در مقابل شهریار و پروین معاصر ترند. دلیل اینکه مجموعه داده را از 4 شاعر مختلف انتخاب کردیم این بود که بتوانیم در انتهای یک تسک کلاس بندی تعریف و حل کنیم.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdff6afc",
      "metadata": {},
      "source": [
        "<h1 style=\"direction:rtl\"><b>نصب و وارد کردن کتابخانه ها</b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "5041a384",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in c:\\users\\mohammad\\.conda\\envs\\cs231n\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in c:\\users\\mohammad\\.conda\\envs\\cs231n\\lib\\site-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in c:\\users\\mohammad\\.conda\\envs\\cs231n\\lib\\site-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\mohammad\\.conda\\envs\\cs231n\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "6d547047",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "\n",
        "import nltk\n",
        "import codecs\n",
        "import random\n",
        "import hazm as hz\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20495294",
      "metadata": {
        "id": "20495294"
      },
      "source": [
        "<h1 style=\"direction:rtl\"><b>نمونه ای متن از هر کدام از شعرا</b></h1>\n",
        "<p style=\"direction:rtl\">در این قسمت چند مصراع از هر کدام از شعرا نشان داده میشود. دیتاست های مربوط به هر کدام از شعرا به این صورت هستند که در ابتدا دو خط اول توضیحی در مورد مجموعه آموزشی است و پس از آن در هر خط یک مصراع آورده شده است. همچنین بعضی از خط ها خالی هستند و هیچ مصراعی را در برندارند به همین دلیل این مصراع های خالی حذف میشوند تا مجموعه آموزشی تمیز باشد.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "5958fd6f",
      "metadata": {
        "id": "5958fd6f",
        "outputId": "2b7afde3-3b3d-47e1-dce7-4d1f946cd625"
      },
      "outputs": [],
      "source": [
        "paths = {\n",
        "    'حافظ': './dataset/hafez.txt',\n",
        "    'پروین': './dataset/parvin.txt',\n",
        "    'صائب': './dataset/saeb.txt',\n",
        "    'شهریار': './dataset/shahriar.txt'\n",
        "}\n",
        "data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "30c1b309",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "Number of mesra: 9435\n",
            "جان درازی تو بادا که یقین می دانم\n",
            "فراق و وصل چه باشد رضای دوست طلب\n",
            "رونق میکده از درس و دعای ما بود\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "Number of mesra: 11145\n",
            "زمانه بسی بیشتر از تو داند\n",
            "تو از فراغ دل و عشرت آمدی بوجود\n",
            "میخرید این گندم ار یک جای کس\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "Number of mesra: 15005\n",
            "چون سیل درین دامن صحرای غریبی\n",
            "بدگمانی لازم بدباطنان افتاده است\n",
            "زین عمارت شد بلند، آوازه نقش جهان\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "Number of mesra: 3813\n",
            "دلی کو شهریارا دشمن جان دوست تر دارد\n",
            "چنگی زن و آفاق پر از شور و نوا کن\n",
            "چنان به خمر و خمار تو خوابناکم و مدهوش\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for poet, path in paths.items():\n",
        "    print(poet)\n",
        "    lines = codecs.open(path, 'rU', 'utf-8').readlines()\n",
        "    lines = lines[2:]\n",
        "    lines = [line.strip() for line in lines]\n",
        "    while '' in lines:\n",
        "        lines.remove('')\n",
        "    print('Number of mesra: {}'.format(len(lines)))\n",
        "    data[poet] = lines\n",
        "    mesra = random.sample(lines, 3)\n",
        "    for item in mesra:\n",
        "        print(item)\n",
        "    print('---------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ed5ba9",
      "metadata": {
        "id": "f9ed5ba9"
      },
      "source": [
        "<h1 style=\"direction:rtl\"><b>نرمالایز کردن متن</b></h1>\n",
        "<p style=\"direction:rtl\">در این قسمت با استفاده از کتابخانه hazm مصراع های شاعران را به فرم نرمال در میاوریم و سپس از هر شاعر یک نمونه شعر قبل و بعد از نرمال کردن را نشان میدهیم. به دلیل اینکه داده های استفاده شده تقریبا تمیز هستند تفاوت زیادی بین نرمال شده و نرمال نشده دیده نمیشود.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "cbd3f0c4",
      "metadata": {
        "id": "cbd3f0c4",
        "outputId": "10dd51c6-43d8-4091-dd24-e9c476bf38fd"
      },
      "outputs": [],
      "source": [
        "normalizer = hz.Normalizer()\n",
        "normalized_data = {}\n",
        "for poet, mesraa in data.items():\n",
        "    nm = [normalizer.normalize(m) for m in mesraa]\n",
        "    normalized_data[poet] = nm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "f142d4a2",
      "metadata": {
        "id": "f142d4a2",
        "outputId": "5ba62b9e-c6da-414a-a276-f624948f6fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "raw: در مقامی که صدارت به فقیران بخشند\n",
            "normalized: در مقامی که صدارت به فقیران بخشند\n",
            "\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "raw: حدیث نور تجلی، بنزد شمع مگوی\n",
            "normalized: حدیث نور تجلی، بنزد شمع مگوی\n",
            "\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "raw: این مصیبت هست بر جا تا بجا ارض و سماست\n",
            "normalized: این مصیبت هست بر جا تا بجا ارض و سماست\n",
            "\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "raw: آنا اویناشیمی غیرت گوزی گورسه کور اولور\n",
            "normalized: آنا اویناشیمی غیرت گوزی گورسه کور اولور\n",
            "\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for poet, mesraa in normalized_data.items():\n",
        "    print(poet)\n",
        "    choices = random.sample(range(len(mesraa)), 1)\n",
        "    for c in choices:\n",
        "        print('raw: {}'.format(data[poet][c]))\n",
        "        print('normalized: {}'.format(normalized_data[poet][c]))\n",
        "        print()\n",
        "    print('---------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab44124d",
      "metadata": {
        "id": "ab44124d"
      },
      "source": [
        "<h1 style=\"direction:rtl\"><b>بدست آوردن توکن ها و اعمال Lemmatization به آنها</b></h1>\n",
        "<p style=\"direction:rtl\">در این قسمت با استفاده از tokenizer موجود در hazm مصراع ها را به کلمات تشکیل دهنده ی آنها میشکنیم تا در ادامه از این توکن های برای پردازش بیشتر متون استفاده کنیم. علاوه بر این کار هر توکن بدست آمده را با استفاده از کتابخانه hazm به یک فرم lemmatize شده میبریم در نهایت برای نمایش آنچه انجام شده یک مصراع نرمال شده همراه با توکن های آن مصراع را برای هر شاعر نمایش میدهیم.</p>\n",
        "<p style=\"direction:rtl\">دلیل اینکه در این تمرین از lemmatizer به جای stemmer استفاده شده است این است که استفاده از lemmatizer فعل ها را به بن آنها میبرد و این موضوع باعث میشود در زمان پیدا کردن کلمات کلیدی عملکرد بهتری داشته باشیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "061a4f1d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "normalized mesraa: به سمع پادشه کامگار ما نرسد\n",
            "tokens: ['به', 'سمع', 'پادشه', 'کامگار', 'ما', 'رسید#رس']\n",
            "\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "normalized mesraa: پا بدر بنهاد و بر دیوار شد\n",
            "tokens: ['پا', 'بدر', 'بنهاد', 'و', 'بر', 'دیوار', 'شد#شو']\n",
            "\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "normalized mesraa: برآرد ز شاخ آنچنان سر شکوفه\n",
            "tokens: ['برآرد', 'ز', 'شاخ', 'آنچنان', 'سر', 'شکوفه']\n",
            "\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "normalized mesraa: ز سبزه چون خط زنگار شاهدان تذهیب\n",
            "tokens: ['ز', 'سبزه', 'چون', 'خط', 'زنگار', 'شاهد', 'تذهیب']\n",
            "\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "tokens = {}\n",
        "lemma = hz.Lemmatizer()\n",
        "\n",
        "for poet, mesraas in normalized_data.items():\n",
        "    tokens[poet] = []\n",
        "    for m in mesraas:\n",
        "        ts = hz.word_tokenize(m)\n",
        "        tokens[poet].append([lemma.lemmatize(t) for t in ts])\n",
        "\n",
        "for poet, mesra_tokenized in tokens.items():\n",
        "    print(poet)\n",
        "    choices = random.sample(range(len(mesra_tokenized)), 1)\n",
        "    for c in choices:\n",
        "        print('normalized mesraa: {}'.format(normalized_data[poet][c]))\n",
        "        print('tokens: {}'.format(tokens[poet][c]))\n",
        "        print()\n",
        "    print('---------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82aea287",
      "metadata": {},
      "source": [
        "<p style=\"direction:rtl\">در این قسمت مشخص میکنیم در هر مصراع از شعر هر کدام از شعرا به طور میانگین چند کلمه وجود دارد.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "2c756df5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "Average number of words in each mesraa: 7.4316905140434555\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "Average number of words in each mesraa: 6.627366532077165\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "Average number of words in each mesraa: 7.474108630456515\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "Average number of words in each mesraa: 7.392866509310254\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for poet, mesra_tokenized in tokens.items():\n",
        "    print(poet)\n",
        "    summation = 0\n",
        "    for m in mesra_tokenized:\n",
        "        summation += len(m)\n",
        "    print('Average number of words in each mesraa: {}'.format(summation/len(mesra_tokenized)))\n",
        "    print('---------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda82477",
      "metadata": {},
      "source": [
        "<h1 style=\"direction:rtl\"><b>حذف کلماتی که تکرار زیادی دارند (stop-words)</b></h1>\n",
        "<p style=\"direction:rtl\">تسک نهایی که بر روی داده ها انجام خواهیم داد یک تسک کلاس بندی خواهد بود که هدف مشخص کردن شاعر یک مصراع داده شده است. برای اینکه عملکرد الگوریتم کلاس بندی بهتر شود یک پیش پردازشی که میتوانیم انجام دهیم حذف stop-word ها است. انجام این کار باعث خواهد شد عملکرد الگوریتم دسته بند بهتر شود.</p>\n",
        "<p style=\"direction:rtl\">برای انجام این کار از یک فایل شامل stop-word های فارسی استفاده میکنیم</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "96f0437d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of stop words = 389\n"
          ]
        }
      ],
      "source": [
        "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('resources/stopwords.txt', 'r', 'utf-8').readlines()]\n",
        "print('Number of stop words = {}'.format(len(stopwords)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76058dda",
      "metadata": {},
      "source": [
        "<p style=\"direction:rtl\">ابتدا اطلاعات مربوط به بسامد لغات قبل از حذف stop-word ها را نشان میدهیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "835140a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "[('که', 2638), ('و', 2442), ('به', 2053), ('از', 1590), ('در', 1430), ('تو', 962), ('ز', 849), ('کرد#کن', 748), ('دل', 741), ('آن', 732)]\n",
            "Total number of words: 70118\n",
            "Average length of words: 3.4707350466356712\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "[('،', 4430), ('و', 3294), ('از', 1567), ('که', 1496), ('را', 1241), ('در', 1078), ('تو', 1072), ('ز', 1020), ('#است', 922), ('چه', 773)]\n",
            "Total number of words: 143980\n",
            "Average length of words: 3.466773162939297\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "[('از', 3851), ('به', 3175), ('که', 2542), ('در', 2502), ('را', 2031), ('ز', 1877), ('،', 1854), ('شد#شو', 1515), ('و', 1340), ('#است', 1286)]\n",
            "Total number of words: 256129\n",
            "Average length of words: 3.5050501895529207\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "[('به', 1044), ('که', 839), ('و', 816), ('از', 623), ('من', 555), ('تو', 439), ('در', 388), ('دل', 274), ('،', 273), ('ای', 257)]\n",
            "Total number of words: 284318\n",
            "Average length of words: 3.517610562820504\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "total_length = 0\n",
        "num_of_words = 0\n",
        "\n",
        "for poet, mesra_tokenized in tokens.items():\n",
        "    print(poet)\n",
        "    fdist = nltk.probability.FreqDist()\n",
        "    for mesraa in tokens[poet]:\n",
        "        for token in mesraa:\n",
        "            total_length += len(token)\n",
        "            num_of_words += 1\n",
        "            fdist[token] += 1\n",
        "    print(fdist.most_common(10))\n",
        "    print('Total number of words: {}'.format(num_of_words))\n",
        "    print('Average length of words: {}'.format(total_length/num_of_words))\n",
        "    print('---------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06694834",
      "metadata": {},
      "source": [
        "<p style=\"direction:rtl\">سپس اطلاعات مربوط به بسامد لغات بعد از حذف stop-word ها را نشان میدهیم. همانطور که میبینیم تعداد لغات در هر کدام از مجموعه ها کاهش چشمیگیری یافته است (زیرا بیشتر لغات همان stop-word ها هستند). همچنین میانگین طول لغات افزایش یافته زیرا تعداد زیادی لغت stop-word حذف شده اند و اندازه این لغات معمولا کوتاه است و بنابراین حذف آنها میانگین اندازه لغات را افزایش داده است.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "270cb059",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "[('ز', 849), ('کرد#کن', 748), ('دل', 741), ('#است', 642), ('بود#باش', 619), ('شد#شو', 507), ('حافظ', 484), ('گفت#گو', 431), ('سر', 431), ('ای', 424)]\n",
            "Total number of words: 47766\n",
            "Average length of words: 4.082966963949253\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "[('،', 4430), ('ز', 1020), ('#است', 922), ('کرد#کن', 712), ('شد#شو', 668), ('بود#باش', 572), ('چو', 487), ('گفت#گو', 362), ('دل', 342), ('کار', 323)]\n",
            "Total number of words: 99555\n",
            "Average length of words: 4.031741248556075\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "[('ز', 1877), ('،', 1854), ('شد#شو', 1515), ('#است', 1286), ('دل', 1134), ('کرد#کن', 1112), ('بود#باش', 798), ('؟', 700), ('داشت#دار', 634), ('گل', 613)]\n",
            "Total number of words: 178501\n",
            "Average length of words: 4.050677587240408\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "[('دل', 274), ('،', 273), ('ای', 257), ('بود#باش', 223), ('کرد#کن', 195), ('آمد#آ', 173), ('#است', 160), ('سر', 158), ('شد#شو', 147), ('شب', 124)]\n",
            "Total number of words: 198401\n",
            "Average length of words: 4.0703827097645675\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Most common words after removing stop words\n",
        "nonstop_tokens = {}\n",
        "total_length = 0\n",
        "num_of_words = 0\n",
        "\n",
        "for poet, mesra_tokenized in tokens.items():\n",
        "    nonstop_tokens[poet] = []\n",
        "    for i in range(len(mesra_tokenized)):\n",
        "        nonstop_tokens[poet].append([t for t in tokens[poet][i] if t not in stopwords])\n",
        "\n",
        "for poet, mesra_tokenized in nonstop_tokens.items():\n",
        "    print(poet)\n",
        "    fdist = nltk.probability.FreqDist()\n",
        "    for mesraa in mesra_tokenized:\n",
        "        for token in mesraa:\n",
        "            total_length += len(token)\n",
        "            num_of_words += 1\n",
        "            fdist[token] += 1\n",
        "    print(fdist.most_common(10))\n",
        "    print('Total number of words: {}'.format(num_of_words))\n",
        "    print('Average length of words: {}'.format(total_length/num_of_words))\n",
        "    print('---------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "9bc96afb",
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_nonstop_data = {}\n",
        "for poet, mesraa in nonstop_tokens.items():\n",
        "    normalized_nonstop_data[poet] = []\n",
        "    for m in mesraa:\n",
        "        normalized_nonstop_data[poet].append(' '.join(m))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac31be9",
      "metadata": {},
      "source": [
        "<h1 style=\"direction:rtl\"><b>به دست آوردن کلمات کلیدی</b></h1>\n",
        "<p style=\"direction:rtl\">پس از اینکه اطلاعات مربوط به بسامد کلمات را بدست آوردیم میتوانیم از این اطلاعات استفاده کنیم و کلمات کلیدی مربوط به هر کدام از داده ها را بدست آوریم. کلمات کلیدی کلماتی هستند که به تعداد دفعات زیادی آمده اند اما stop-word نیستند. همچنین طول آنها نیز نباید خیلی کم باشد.</p>\n",
        "<p style=\"direction:rtl\">یک نکته مهم در مورد پیدا کردن کلمات کلیدی این است که در زمان lemmatize کردن فعلا ها به بن ماضی و مضارع تبدیل شده اند و این بن های با علامت # از هم جدا شده اند به این ترتیب کلماتی که این علامت را دارا هستند را حذف میکنیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "432f7be5",
      "metadata": {
        "id": "432f7be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حافظ\n",
            "keywords\n",
            "[('میخانه', 55), ('شیرین', 53), ('میکده', 49), ('خورشید', 47), ('کجاست', 44), ('جانان', 40), ('خواجه', 40), ('خرابات', 36), ('حکایت', 34), ('مشکین', 33)]\n",
            "---------------------------------------------------\n",
            "پروین\n",
            "keywords\n",
            "[('روزگار', 67), ('زمانه', 56), ('پروین', 55), ('امروز', 46), ('پنهان', 43), ('حقیقت', 38), ('گردون', 37), ('مسکین', 36), ('آلوده', 34), ('باغبان', 34)]\n",
            "---------------------------------------------------\n",
            "صائب\n",
            "keywords\n",
            "[('آفتاب', 230), ('خورشید', 171), ('آیینه', 170), ('روزگار', 126), ('آسمان', 124), ('شکوفه', 103), ('دامان', 82), ('دیوار', 80), ('نوبهار', 78), ('گلستان', 76)]\n",
            "---------------------------------------------------\n",
            "شهریار\n",
            "keywords\n",
            "[('شهریار', 102), ('شهریارا', 56), ('شیراز', 28), ('اولماز', 24), ('پروانه', 23), ('خورشید', 23), ('الیندن', 23), ('اولوب', 22), ('شیرین', 21), ('زندان', 21)]\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for poet, mesra_tokenized in nonstop_tokens.items():\n",
        "    print(poet)\n",
        "    keywords = nltk.probability.FreqDist()\n",
        "    for mesraa in mesra_tokenized:\n",
        "        for token in mesraa:\n",
        "            if (len(token) < 5) or ('#' in token):\n",
        "                continue\n",
        "            keywords[token] += 1\n",
        "    print('keywords')\n",
        "    print(keywords.most_common(10))\n",
        "    print('---------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ffe421a",
      "metadata": {},
      "source": [
        "<h1 style=\"direction:rtl\"><b>کلاس بندی با استفاده از Naive Bayes</b></h1>\n",
        "<p style=\"direction:rtl\">پس از اینکه روی متون پیش پردازشات را انجام دادیم وقت آن است که از این متن ها برای یک مسئله کلاسیفیکیشن استفاده کنیم. در این قسمت دیتای مربوط به شعرا را به دو قسمت تقسیم میکنیم. داده های آموزشی و داده های تست در هر کدام از دو مجموعه شعر های هر 4 شاعر موجود است. هدف این است که با استفاده از داده های آموزشی یک مدل Naive Bayes برای کلاس بندی آموزش دهیم و سپس روی داده های تست مشخص کنیم هر مصراع یا بیت مربوط به کدام شاعر است. در نهایت نتایج را به صورت کمی گزارش میکنیم.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9243603",
      "metadata": {},
      "source": [
        "<p style=\"direction:rtl\">ابتدا در این بخش تابع create_train_test_dataset را تعریف میکنیم که همانطور که از اسمش مشخص است داده ها را به دو بخش آموزش و تست تقسیم میکند</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "a7a55fe1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_train_test_dataset(data_dic):\n",
        "    temp_dic = {'mesraa': [], 'target': []}\n",
        "    labels = {\n",
        "        'حافظ': 0,\n",
        "        'پروین': 1,\n",
        "        'صائب': 2,\n",
        "        'شهریار': 3\n",
        "    }\n",
        "    for poet, mesraa in normalized_data.items():\n",
        "        for m in mesraa:\n",
        "            temp_dic['mesraa'].append(m)\n",
        "            temp_dic['target'].append(labels[poet])\n",
        "    df = pd.DataFrame(data=temp_dic)\n",
        "    train, test = train_test_split(df, test_size=0.2)\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a37dcb",
      "metadata": {},
      "source": [
        "<p style=\"direction:rtl\">تابعی که در ادامه نوشته شده است یک مجموعه آموزشی و یک مجموعه تست را به عنوان ورودی میگیرد و سپس با استفاده از داده های اموزشی یک مدل Naive Bayes آموزش میدهد و سپس روی داده های تست ارزیابی را انجام میدهد.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "7bc55b97",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_and_evaluate(train, test):\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 2)).fit(train['mesraa'])\n",
        "    X_train_vectorized = vectorizer.transform(train['mesraa'])\n",
        "    model = MultinomialNB(alpha=0.1)\n",
        "    model.fit(X_train_vectorized, train['target'].values)\n",
        "    predictions = model.predict(vectorizer.transform(test['mesraa']))\n",
        "    print(\"Accuracy:\", 100 * sum(predictions == test['target'].values) / len(predictions), '%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "afa4c73a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw texts:\n",
            "Accuracy: 76.05329949238579 %\n",
            "----------------------------------\n",
            "Normalized texts:\n",
            "Accuracy: 77.29695431472081 %\n",
            "----------------------------------\n",
            "Normalized non-stop texts:\n",
            "Accuracy: 77.13197969543147 %\n"
          ]
        }
      ],
      "source": [
        "train, test = create_train_test_dataset(data)\n",
        "print('Raw texts:')\n",
        "fit_and_evaluate(train, test)\n",
        "print('----------------------------------')\n",
        "train, test = create_train_test_dataset(normalized_data)\n",
        "print('Normalized texts:')\n",
        "fit_and_evaluate(train, test)\n",
        "print('----------------------------------')\n",
        "train, test = create_train_test_dataset(normalized_nonstop_data)\n",
        "print('Normalized non-stop texts:')\n",
        "fit_and_evaluate(train, test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e550f27",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
