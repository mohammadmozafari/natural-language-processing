{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from hazm import Normalizer, Stemmer, Lemmatizer, word_tokenize\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepare the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.dat','r','utf-8').readlines()]\n",
    "lemmatizer = Lemmatizer()\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beyt</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الا یا ایها الساقی ادر کاسا و ناولها که عشق آس...</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>به بوی نافه ای کاخر صبا زان طره بگشاید ز تاب ج...</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مرا در منزل جانان چه امن عیش چون هر دم جرس فری...</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>به می سجاده رنگین کن گرت پیر مغان گوید که سالک...</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شب تاریک و بیم موج و گردابی چنین هایل کجا دانن...</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20402</th>\n",
       "      <td>ف و ما اعتدی الا علی من یعتدی بشر الینا بالرجا...</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20403</th>\n",
       "      <td>و تقایض الدنیا بدولت سرمد مهمارجوت رجوت خیرالم...</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20404</th>\n",
       "      <td>و اذا قصدت قصدت خیرالمقصد مدت حیوت الناس تحت ظ...</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20405</th>\n",
       "      <td>لا زال فی اهنی الحیوت و ارغد هذی خلال الزاکیات...</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20406</th>\n",
       "      <td>لمحمد بن محمدبن محمد او یحسب الانسان ماسلک اهتدی</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20407 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    beyt author\n",
       "0      الا یا ایها الساقی ادر کاسا و ناولها که عشق آس...  hafez\n",
       "1      به بوی نافه ای کاخر صبا زان طره بگشاید ز تاب ج...  hafez\n",
       "2      مرا در منزل جانان چه امن عیش چون هر دم جرس فری...  hafez\n",
       "3      به می سجاده رنگین کن گرت پیر مغان گوید که سالک...  hafez\n",
       "4      شب تاریک و بیم موج و گردابی چنین هایل کجا دانن...  hafez\n",
       "...                                                  ...    ...\n",
       "20402  ف و ما اعتدی الا علی من یعتدی بشر الینا بالرجا...  saadi\n",
       "20403  و تقایض الدنیا بدولت سرمد مهمارجوت رجوت خیرالم...  saadi\n",
       "20404  و اذا قصدت قصدت خیرالمقصد مدت حیوت الناس تحت ظ...  saadi\n",
       "20405  لا زال فی اهنی الحیوت و ارغد هذی خلال الزاکیات...  saadi\n",
       "20406   لمحمد بن محمدبن محمد او یحسب الانسان ماسلک اهتدی  saadi\n",
       "\n",
       "[20407 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = r'./Persian_poems_corpus/normalized/'                                                   # use your path\n",
    "path = './subset_corpus/'\n",
    "all_files = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    poet = filename.replace('\\\\', '/').split('/')[-1][:-9]\n",
    "    if poet == 'khaghani':\n",
    "        continue\n",
    "    df = pd.read_csv(filename, names=['mesra'], header=None)\n",
    "    df['beyt index'] = df.index // 2\n",
    "    df = df.groupby(['beyt index']).agg({'mesra': ' '.join})\n",
    "    df = df.rename(columns={'mesra':'beyt'})\n",
    "    df['author'] = poet\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "df[['beyt']].to_csv('./corpus.csv', index=False)\n",
    "# df.beyt = df.beyt.apply(preprocess)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer):\n",
    "        rows = []\n",
    "    \n",
    "        for row in df.itertuples():\n",
    "            tokenized = tokenizer(row.beyt, padding='max_length', truncation='longest_first', max_length=30)\n",
    "            rows.append(tokenized)\n",
    "                \n",
    "        self.__rows = rows\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__rows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.__rows[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e4ead6903a3eb641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\mozaf\\.cache\\huggingface\\datasets\\csv\\default-e4ead6903a3eb641\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\mozaf\\.cache\\huggingface\\datasets\\csv\\default-e4ead6903a3eb641\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beyt': 'ز روی دوست دل دشمنان چه دریابد چراغ مرده کجا شمع آفتاب کجا'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('csv', data_files='./corpus.csv')\n",
    "ds['train'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define the model and start training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = './trained_models/HooshvareLab_bert-fa-zwnj-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20407ex [00:06, 3135.60ex/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = ds.map(lambda x: tokenizer(x['beyt']), batched=False, num_proc=1, remove_columns=['beyt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  607,\n",
       "  2126,\n",
       "  2966,\n",
       "  2266,\n",
       "  9198,\n",
       "  2180,\n",
       "  31684,\n",
       "  5603,\n",
       "  8419,\n",
       "  6079,\n",
       "  12494,\n",
       "  595,\n",
       "  1954,\n",
       "  1949,\n",
       "  6079,\n",
       "  3],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  7.10ba/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = './trained_models/HooshvareLab_bert-fa-zwnj-base'\n",
    "# df = pd.read_csv('./corpus.csv')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# dataset = PoemsDataset(df, tokenizer)\n",
    "\n",
    "# dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = './trained_models/HooshvareLab_bert-fa-zwnj-base'\n",
    "model = AutoModelForMaskedLM.from_pretrained('./trained_models/HooshvareLab_bert-fa-zwnj-base')\n",
    "\n",
    "# bert_base = models.Transformer('./trained_models/HooshvareLab_bert-fa-zwnj-base')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# tokenized_datasets = ds.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"beyt\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python Venv\\mfi\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2919\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3650\n",
      "  0%|          | 0/3650 [00:00<?, ?it/s]E:\\Python Venv\\mfi\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 8000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "  0%|          | 1/3650 [00:06<6:39:24,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9326, 'learning_rate': 4.998630136986302e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6560\\2473599308.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Python Venv\\mfi\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1438\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1440\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1441\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python Venv\\mfi\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1563\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1565\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python Venv\\mfi\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_memory_tracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2204\u001b[1;33m         \u001b[0meval_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_eval_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2205\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python Venv\\mfi\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mget_eval_dataloader\u001b[1;34m(self, eval_dataset)\u001b[0m\n\u001b[0;32m    715\u001b[0m         \"\"\"\n\u001b[0;32m    716\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_dataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trainer: evaluation requires an eval_dataset.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0meval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./trained_models/bert_finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    evaluation_strategy='steps',\n",
    "    no_cuda = True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    disable_tqdm=False,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets['train']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "442e29bcd04bf821d0544d72aba6c1125df328429626c0407e477187cbe68c47"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('mfi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
