{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import models, SentenceTransformer, util, CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Download all models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\Python Venv\\mfi\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 8000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HooshvareLab/bert-fa-zwnj-base downloaded.\n",
      "m3hrdadfi/bert-fa-base-uncased-wikinli downloaded.\n"
     ]
    }
   ],
   "source": [
    "_ = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')\n",
    "print('HooshvareLab/bert-fa-zwnj-base downloaded.')\n",
    "\n",
    "_ = CrossEncoder('m3hrdadfi/bert-fa-base-uncased-wikinli')\n",
    "print('m3hrdadfi/bert-fa-base-uncased-wikinli downloaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10228/10228 [00:00<00:00, 244422.59it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "paths = glob.glob('./corpus/*.txt')\n",
    "for path in paths:\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            if len(line) < 5:\n",
    "                continue\n",
    "            data.append(line)\n",
    "\n",
    "data = data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vanilla BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load deep model\n",
    "encoder = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')\n",
    "\n",
    "# Compute the embedding of all data\n",
    "corpus_embeddings = encoder.encode(data, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "\t ده روزه مهر گردون افسانه است و افسون\n",
      "\n",
      "-------------------------------------------\n",
      "Result 2:\n",
      "\t ای صاحب کرامت شکرانه سلامت\n",
      "\n",
      "-------------------------------------------\n",
      "Result 3:\n",
      "\t فغان کاین لولیان شوخ شیرین کار شهرآشوب\n",
      "\n",
      "-------------------------------------------\n",
      "Result 4:\n",
      "\t کجاست دیر مغان و شراب ناب کجا\n",
      "\n",
      "-------------------------------------------\n",
      "Result 5:\n",
      "\t نصیحت گوش کن جانا که از جان دوست تر دارند\n",
      "\n",
      "-------------------------------------------\n",
      "Result 6:\n",
      "\t جوانان سعادتمند پند پیر دانا را\n",
      "\n",
      "-------------------------------------------\n",
      "Result 7:\n",
      "\t ز عشق ناتمام ما جمال یار مستغنی است\n",
      "\n",
      "-------------------------------------------\n",
      "Result 8:\n",
      "\t بشد که یاد خوشش باد روزگار وصال\n",
      "\n",
      "-------------------------------------------\n",
      "Result 9:\n",
      "\t ای شیخ پاکدامن معذور دار ما را\n",
      "\n",
      "-------------------------------------------\n",
      "Result 10:\n",
      "\t جواب تلخ می زیبد لب لعل شکرخا را\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = 'شراب'\n",
    "\n",
    "query_embedding = encoder.encode(query, convert_to_tensor=True)\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0, :]\n",
    "cos_scores = cos_scores.cpu().detach().numpy()\n",
    "most_relevant = np.argsort(cos_scores)[::-1]\n",
    "\n",
    "for i in range(10):\n",
    "    print('Result {}:'.format(i + 1))\n",
    "    print('\\t', hems[most_relevant[i]])\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT with CrossEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\mozaf/.cache\\torch\\sentence_transformers\\HooshvareLab_bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load deep model\n",
    "encoder = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')\n",
    "\n",
    "# Compute the embedding of all data\n",
    "corpus_embeddings = encoder.encode(data, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'corpus_id': 34, 'score': 0.5476308465003967, 'cross-score': 0.9781284}, {'corpus_id': 17, 'score': 0.546454668045044, 'cross-score': 0.9260603}, {'corpus_id': 26, 'score': 0.5148657560348511, 'cross-score': 0.89803964}, {'corpus_id': 68, 'score': 0.5676670074462891, 'cross-score': -3.099187}, {'corpus_id': 89, 'score': 0.5120704174041748, 'cross-score': -3.2056913}, {'corpus_id': 72, 'score': 0.5576296448707581, 'cross-score': -3.528982}, {'corpus_id': 36, 'score': 0.5159090161323547, 'cross-score': -3.5747998}, {'corpus_id': 41, 'score': 0.5111278295516968, 'cross-score': -4.1192136}, {'corpus_id': 42, 'score': 0.5260371565818787, 'cross-score': -4.1353583}, {'corpus_id': 43, 'score': 0.521490216255188, 'cross-score': -4.1780953}]\n",
      "Result 1:\n",
      "\t فغان کاین لولیان شوخ شیرین کار شهرآشوب\n",
      "\n",
      "-------------------------------------------\n",
      "Result 2:\n",
      "\t کجاست دیر مغان و شراب ناب کجا\n",
      "\n",
      "-------------------------------------------\n",
      "Result 3:\n",
      "\t بشد که یاد خوشش باد روزگار وصال\n",
      "\n",
      "-------------------------------------------\n",
      "Result 4:\n",
      "\t ده روزه مهر گردون افسانه است و افسون\n",
      "\n",
      "-------------------------------------------\n",
      "Result 5:\n",
      "\t ای شیخ پاکدامن معذور دار ما را\n",
      "\n",
      "-------------------------------------------\n",
      "Result 6:\n",
      "\t ای صاحب کرامت شکرانه سلامت\n",
      "\n",
      "-------------------------------------------\n",
      "Result 7:\n",
      "\t ز عشق ناتمام ما جمال یار مستغنی است\n",
      "\n",
      "-------------------------------------------\n",
      "Result 8:\n",
      "\t جواب تلخ می زیبد لب لعل شکرخا را\n",
      "\n",
      "-------------------------------------------\n",
      "Result 9:\n",
      "\t نصیحت گوش کن جانا که از جان دوست تر دارند\n",
      "\n",
      "-------------------------------------------\n",
      "Result 10:\n",
      "\t جوانان سعادتمند پند پیر دانا را\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = 'شراب'\n",
    "\n",
    "query_embedding = encoder.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=10)\n",
    "hits = hits[0]\n",
    "\n",
    "cross_encoder = CrossEncoder('m3hrdadfi/bert-fa-base-uncased-wikinli')\n",
    "cross_inp = [[query, data[hit['corpus_id']]] for hit in hits]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx][1]\n",
    "re_ranked = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print('Result {}:'.format(i + 1))\n",
    "    print('\\t', data[re_ranked[i]['corpus_id']])\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT Weighted Sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "442e29bcd04bf821d0544d72aba6c1125df328429626c0407e477187cbe68c47"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('mfi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
